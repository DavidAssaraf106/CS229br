{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ARM",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeiw-k3SxdoW"
      },
      "source": [
        "!pip install pytorch_model_summary\r\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY6qXEPjxFGu"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import torch\r\n",
        "from sklearn.datasets import load_digits\r\n",
        "from sklearn import datasets\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import wandb\r\n",
        "from pytorch_model_summary import summary"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpLjQD9LxcM9"
      },
      "source": [
        "class Digits(Dataset):\r\n",
        "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\r\n",
        "    def __init__(self, mode='train', transforms=None):\r\n",
        "        digits = load_digits()\r\n",
        "        if mode == 'train':\r\n",
        "            self.data = digits.data[:1000].astype(np.float32)\r\n",
        "        elif mode == 'val':\r\n",
        "            self.data = digits.data[1000:1350].astype(np.float32)\r\n",
        "        else:\r\n",
        "            self.data = digits.data[1350:].astype(np.float32)\r\n",
        "\r\n",
        "        self.transforms = transforms\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.data)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        sample = self.data[idx]\r\n",
        "        if self.transforms:\r\n",
        "            sample = self.transforms(sample)\r\n",
        "        return sample"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXh-bhumxyas"
      },
      "source": [
        "class CausalConv1d(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    A causal 1D convolution.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, dilation, A=False, **kwargs):\r\n",
        "        super(CausalConv1d, self).__init__()\r\n",
        "\r\n",
        "        # attributes:\r\n",
        "        self.kernel_size = kernel_size\r\n",
        "        self.dilation = dilation\r\n",
        "        self.A = A\r\n",
        "\r\n",
        "        # module:\r\n",
        "        self.conv1d = torch.nn.Conv1d(in_channels, out_channels,\r\n",
        "                                      kernel_size, stride=1,\r\n",
        "                                      padding=(kernel_size - 1 + A * 1) * dilation,\r\n",
        "                                      dilation=dilation,\r\n",
        "                                      **kwargs)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        conv1d_out = self.conv1d(x)\r\n",
        "        # remove several last values from the end\r\n",
        "        return conv1d_out[:, :, :-(self.kernel_size - 1 + self.A * 2) * self.dilation]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwmIggGAx9Z7"
      },
      "source": [
        "EPS = 1.e-5\r\n",
        "\r\n",
        "def log_categorical(x, p, num_classes=256, reduction=None, dim=None):\r\n",
        "    x_one_hot = F.one_hot(x.long(), num_classes=num_classes)\r\n",
        "    log_p = x_one_hot * torch.log(torch.clamp(p, EPS, 1. - EPS))\r\n",
        "    if reduction == 'avg':\r\n",
        "        return torch.mean(log_p, dim)\r\n",
        "    elif reduction == 'sum':\r\n",
        "        return torch.sum(log_p, dim)\r\n",
        "    else:\r\n",
        "        return log_p"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFLIhzi9yK7S"
      },
      "source": [
        "class ARM(nn.Module):   \r\n",
        "    def __init__(self, net, D=2, num_vals=256):\r\n",
        "        super(ARM, self).__init__()\r\n",
        "        print('ARM by DA.')\r\n",
        "        self.net = net\r\n",
        "        self.num_vals = num_vals\r\n",
        "        self.D = D\r\n",
        "\r\n",
        "    def f(self, x):\r\n",
        "        h = self.net(x.unsqueeze(1))\r\n",
        "        h = h.permute(0, 2, 1)\r\n",
        "        p = torch.softmax(h, 2)\r\n",
        "        return p\r\n",
        "        \r\n",
        "    def forward(self, x, reduction='avg'):\r\n",
        "        if reduction == 'avg':\r\n",
        "            return -(self.log_prob(x).mean())\r\n",
        "        elif reduction == 'sum':\r\n",
        "            return -(self.log_prob(x).sum())\r\n",
        "        else:\r\n",
        "            raise ValueError('reduction could be either `avg` or `sum`.')\r\n",
        "\r\n",
        "    def log_prob(self, x):\r\n",
        "        mu_d = self.f(x)\r\n",
        "        log_p = log_categorical(x, mu_d, num_classes=self.num_vals, reduction='sum', dim=-1).sum(-1)\r\n",
        "        \r\n",
        "        return log_p\r\n",
        "\r\n",
        "    def sample(self, batch_size):\r\n",
        "        x_new = torch.zeros((batch_size, self.D)).cuda()\r\n",
        "        for d in range(self.D):\r\n",
        "            p = self.f(x_new)\r\n",
        "            x_new_d = torch.multinomial(p[:, d, :], num_samples=1)\r\n",
        "            x_new[:, d] = x_new_d[:,0]\r\n",
        "        return x_new"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuyFUO84yozn"
      },
      "source": [
        "def evaluation(test_loader, name=None, model_best=None, epoch=None):\r\n",
        "    # EVALUATION\r\n",
        "    if model_best is None:\r\n",
        "        # load best performing model\r\n",
        "        model_best = torch.load(name + '.model')\r\n",
        "    model_best.eval()\r\n",
        "    model_best.cuda()\r\n",
        "    loss = 0.\r\n",
        "    N = 0.\r\n",
        "    for indx_batch, test_batch in enumerate(test_loader):\r\n",
        "        test_batch = test_batch.cuda()\r\n",
        "        loss_t = model_best.forward(test_batch, reduction='sum')\r\n",
        "        loss = loss + loss_t.item()\r\n",
        "        N = N + test_batch.shape[0]\r\n",
        "    loss = loss / N\r\n",
        "    if epoch is None:\r\n",
        "        print(f'FINAL LOSS: nll={loss}')\r\n",
        "    else:\r\n",
        "        print(f'Epoch: {epoch}, val nll={loss}')\r\n",
        "    return loss\r\n",
        "\r\n",
        "\r\n",
        "def samples_real(name, test_loader):\r\n",
        "    # REAL-------\r\n",
        "    num_x = 4\r\n",
        "    num_y = 4\r\n",
        "    x = next(iter(test_loader)).detach().numpy()\r\n",
        "    fig, ax = plt.subplots(num_x, num_y)\r\n",
        "    for i, ax in enumerate(ax.flatten()):\r\n",
        "        plottable_image = np.reshape(x[i], (8, 8))\r\n",
        "        ax.imshow(plottable_image, cmap='gray')\r\n",
        "        ax.axis('off')\r\n",
        "    plt.savefig(name+'_real_images.pdf', bbox_inches='tight')\r\n",
        "    plt.close()\r\n",
        "\r\n",
        "\r\n",
        "def samples_generated(name, data_loader, extra_name=''):\r\n",
        "    x = next(iter(data_loader)).detach().numpy()\r\n",
        "    # GENERATIONS-------\r\n",
        "    model_best = torch.load(name + '.model')\r\n",
        "    model_best.eval()\r\n",
        "    num_x = 4\r\n",
        "    num_y = 4\r\n",
        "    x = model_best.sample(num_x * num_y)\r\n",
        "    x = x.cpu().detach().numpy()\r\n",
        "    fig, ax = plt.subplots(num_x, num_y)\r\n",
        "    for i, ax in enumerate(ax.flatten()):\r\n",
        "        plottable_image = np.reshape(x[i], (8, 8))\r\n",
        "        ax.imshow(plottable_image, cmap='gray')\r\n",
        "        ax.axis('off')\r\n",
        "    plt.savefig(name + '_generated_images' + extra_name + '.pdf', bbox_inches='tight')\r\n",
        "    plt.close()\r\n",
        "\r\n",
        "\r\n",
        "def plot_curve(name, nll_val):\r\n",
        "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\r\n",
        "    plt.xlabel('epochs')\r\n",
        "    plt.ylabel('nll')\r\n",
        "    plt.savefig(name + '_nll_val_curve.pdf', bbox_inches='tight')\r\n",
        "    plt.close()\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okZrt5Nmy1rg"
      },
      "source": [
        "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader):\r\n",
        "    nll_val = []\r\n",
        "    best_nll = 1000.\r\n",
        "    patience = 0\r\n",
        "\r\n",
        "    # Main loop\r\n",
        "    for e in range(num_epochs):\r\n",
        "        # TRAINING\r\n",
        "        model.train()\r\n",
        "        model.cuda()\r\n",
        "        for indx_batch, batch in enumerate(training_loader):\r\n",
        "            batch = batch.cuda()\r\n",
        "            loss = model.forward(batch)\r\n",
        "\r\n",
        "            optimizer.zero_grad()\r\n",
        "            loss.backward(retain_graph=True)\r\n",
        "            optimizer.step()\r\n",
        "        # Validation\r\n",
        "        loss_val = evaluation(val_loader, model_best=model, epoch=e)\r\n",
        "        nll_val.append(loss_val)  # save for plotting\r\n",
        "\r\n",
        "        if e == 0:\r\n",
        "            print('saved!')\r\n",
        "            torch.save(model, name + '.model')\r\n",
        "            best_nll = loss_val\r\n",
        "        else:\r\n",
        "            if loss_val < best_nll:\r\n",
        "                print('saved!')\r\n",
        "                torch.save(model, name + '.model')\r\n",
        "                best_nll = loss_val\r\n",
        "                patience = 0\r\n",
        "\r\n",
        "                samples_generated(name, val_loader, extra_name=\"_epoch_\" + str(e))\r\n",
        "            else:\r\n",
        "                patience = patience + 1\r\n",
        "\r\n",
        "        if patience > max_patience:\r\n",
        "            break\r\n",
        "\r\n",
        "    nll_val = np.asarray(nll_val)\r\n",
        "\r\n",
        "    return nll_val"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFMh9_moy6Cz"
      },
      "source": [
        "train_data = Digits(mode='train')\r\n",
        "val_data = Digits(mode='val')\r\n",
        "test_data = Digits(mode='test')\r\n",
        "\r\n",
        "training_loader = DataLoader(train_data, batch_size=64, shuffle=True)\r\n",
        "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\r\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\r\n",
        "\r\n",
        "result_dir = 'results/'\r\n",
        "if not(os.path.exists(result_dir)):\r\n",
        "    os.mkdir(result_dir)\r\n",
        "name = 'arm'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNzqT7d7zWCY"
      },
      "source": [
        "D = 64   # input dimension\r\n",
        "M = 256  # the number of neurons in scale (s) and translation (t) nets\r\n",
        "\r\n",
        "lr = 1e-3 # learning rate\r\n",
        "num_epochs = 1000 # max. number of epochs\r\n",
        "max_patience = 20 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19nhv7khziim",
        "outputId": "285925e2-6c86-4c60-c8f2-5c495b0e1b51"
      },
      "source": [
        "likelihood_type = 'categorical'\r\n",
        "\r\n",
        "num_vals = 17\r\n",
        "\r\n",
        "kernel = 7\r\n",
        "\r\n",
        "net = nn.Sequential(\r\n",
        "    CausalConv1d(in_channels=1, out_channels=M, dilation=1, kernel_size=kernel, A=True, bias=True),\r\n",
        "    nn.LeakyReLU(),\r\n",
        "    CausalConv1d(in_channels=M, out_channels=M, dilation=1, kernel_size=kernel, A=False, bias=True),\r\n",
        "    nn.LeakyReLU(),\r\n",
        "    CausalConv1d(in_channels=M, out_channels=M, dilation=1, kernel_size=kernel, A=False, bias=True),\r\n",
        "    nn.LeakyReLU(),\r\n",
        "    CausalConv1d(in_channels=M, out_channels=num_vals, dilation=1, kernel_size=kernel, A=False, bias=True))\r\n",
        "\r\n",
        "model = ARM(net, D=D, num_vals=num_vals)\r\n",
        "\r\n",
        "# Print the summary (like in Keras)\r\n",
        "print(summary(model, torch.zeros(1, 64), show_input=False, show_hierarchical=False))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ARM by DA.\n",
            "-----------------------------------------------------------------------\n",
            "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
            "=======================================================================\n",
            "    CausalConv1d-1        [1, 256, 64]           2,048           2,048\n",
            "       LeakyReLU-2        [1, 256, 64]               0               0\n",
            "    CausalConv1d-3        [1, 256, 64]         459,008         459,008\n",
            "       LeakyReLU-4        [1, 256, 64]               0               0\n",
            "    CausalConv1d-5        [1, 256, 64]         459,008         459,008\n",
            "       LeakyReLU-6        [1, 256, 64]               0               0\n",
            "    CausalConv1d-7         [1, 17, 64]          30,481          30,481\n",
            "=======================================================================\n",
            "Total params: 950,545\n",
            "Trainable params: 950,545\n",
            "Non-trainable params: 0\n",
            "-----------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMUog_AGz0F9"
      },
      "source": [
        "# OPTIMIZER\r\n",
        "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4UWCnqoz16w",
        "outputId": "63400380-ce79-4d8b-d41c-a6fd2e86db66"
      },
      "source": [
        "run = wandb.init(project=\"CS229br_AMR_CausalConv1D\", reinit=True)\r\n",
        "nll_val = training(name=result_dir + name, max_patience=max_patience, num_epochs=num_epochs, model=model, optimizer=optimizer,\r\n",
        "                       training_loader=training_loader, val_loader=val_loader)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, val nll=106.55981026785715\n",
            "saved!\n",
            "Epoch: 1, val nll=104.40988630022322\n",
            "saved!\n",
            "Epoch: 2, val nll=101.85582170758929\n",
            "saved!\n",
            "Epoch: 3, val nll=99.39316545758929\n",
            "saved!\n",
            "Epoch: 4, val nll=97.5657958984375\n",
            "saved!\n",
            "Epoch: 5, val nll=96.1687353515625\n",
            "saved!\n",
            "Epoch: 6, val nll=95.08131417410715\n",
            "saved!\n",
            "Epoch: 7, val nll=94.30946986607142\n",
            "saved!\n",
            "Epoch: 8, val nll=93.67075962611607\n",
            "saved!\n",
            "Epoch: 9, val nll=93.39017578125\n",
            "saved!\n",
            "Epoch: 10, val nll=92.91685407366072\n",
            "saved!\n",
            "Epoch: 11, val nll=92.5396630859375\n",
            "saved!\n",
            "Epoch: 12, val nll=92.13993233816964\n",
            "saved!\n",
            "Epoch: 13, val nll=91.84229910714286\n",
            "saved!\n",
            "Epoch: 14, val nll=91.83565150669642\n",
            "saved!\n",
            "Epoch: 15, val nll=91.44882603236607\n",
            "saved!\n",
            "Epoch: 16, val nll=91.53663992745535\n",
            "Epoch: 17, val nll=90.92118443080358\n",
            "saved!\n",
            "Epoch: 18, val nll=91.10525669642857\n",
            "Epoch: 19, val nll=90.84852329799106\n",
            "saved!\n",
            "Epoch: 20, val nll=90.89668038504465\n",
            "Epoch: 21, val nll=90.26738420758929\n",
            "saved!\n",
            "Epoch: 22, val nll=90.626083984375\n",
            "Epoch: 23, val nll=90.35715262276786\n",
            "Epoch: 24, val nll=90.07863071986607\n",
            "saved!\n",
            "Epoch: 25, val nll=90.41009556361607\n",
            "Epoch: 26, val nll=89.98472935267857\n",
            "saved!\n",
            "Epoch: 27, val nll=89.80929408482143\n",
            "saved!\n",
            "Epoch: 28, val nll=89.97275041852679\n",
            "Epoch: 29, val nll=89.49594447544642\n",
            "saved!\n",
            "Epoch: 30, val nll=89.79051339285714\n",
            "Epoch: 31, val nll=89.60139927455357\n",
            "Epoch: 32, val nll=89.24750837053571\n",
            "saved!\n",
            "Epoch: 33, val nll=89.61202357700893\n",
            "Epoch: 34, val nll=89.10921665736608\n",
            "saved!\n",
            "Epoch: 35, val nll=89.62739397321428\n",
            "Epoch: 36, val nll=89.27861328125\n",
            "Epoch: 37, val nll=89.19292689732143\n",
            "Epoch: 38, val nll=89.39599748883928\n",
            "Epoch: 39, val nll=89.41820452008929\n",
            "Epoch: 40, val nll=89.29626395089285\n",
            "Epoch: 41, val nll=89.19054827008928\n",
            "Epoch: 42, val nll=89.66328194754465\n",
            "Epoch: 43, val nll=89.25377650669643\n",
            "Epoch: 44, val nll=89.14163504464285\n",
            "Epoch: 45, val nll=89.61073939732142\n",
            "Epoch: 46, val nll=89.49906110491071\n",
            "Epoch: 47, val nll=89.5722509765625\n",
            "Epoch: 48, val nll=89.26765066964286\n",
            "Epoch: 49, val nll=89.51293666294643\n",
            "Epoch: 50, val nll=89.17186453683036\n",
            "Epoch: 51, val nll=89.57775390625\n",
            "Epoch: 52, val nll=89.70828264508928\n",
            "Epoch: 53, val nll=89.87752580915179\n",
            "Epoch: 54, val nll=90.00640206473214\n",
            "Epoch: 55, val nll=89.77275739397321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx-Yg0cl0Rnm",
        "outputId": "5eb08a13-2e5a-4d28-97ec-44c609b3cbe9"
      },
      "source": [
        "test_loss = evaluation(name=result_dir + name, test_loader=test_loader)\r\n",
        "f = open(result_dir + name + '_test_loss.txt', \"w\")\r\n",
        "f.write(str(test_loss))\r\n",
        "f.close()\r\n",
        "\r\n",
        "samples_real(result_dir + name, test_loader)\r\n",
        "\r\n",
        "plot_curve(result_dir + name, nll_val)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FINAL LOSS: nll=86.34433768526286\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}